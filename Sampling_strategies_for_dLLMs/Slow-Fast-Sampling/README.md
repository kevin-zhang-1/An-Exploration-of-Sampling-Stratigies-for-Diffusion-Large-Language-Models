# Accelerating Diffusion Large Language Models with SlowFast Sampling: The Three Golden Principles
Official PyTorch implementation of the paper "[Accelerating Diffusion Large Language Models with SlowFast Sampling: The Three Golden Principles](docs/SlowFastSampling.pdf)" (**Slow Fast Sampling**).  

***The Three Golden Principles: Certainty Â· Convergence Â· Positional***

<!-- [![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/Python-3.10%2B-yellow.svg)](#installation)
[![Paper](https://img.shields.io/badge/arXiv-2406.12345-b31b1b.svg)](https://arxiv.org/abs/2406.12345) -->

<div align="center">

![Pipeline](assets/speed.jpg)  
**Fig. 1 â€“ Throughput and Accuracy Comparison on GPQA (8-shot, Length=1024) with LLaDA and Our Proposed Methods.**

</div>

<!-- --- -->

<!-- ## ğŸ”¥ News -->
<!-- | Date | Update |
|------|--------|
| **2025-06-12** | **SFS** is now the default sampler in **LLaDA-V** |
| 2025-06-01 | Open-sourced the full PyTorch implementation & reproducible scripts |
| 2025-05-30 | Pre-print released on arXiv | -->

---

## âœ¨ Key Highlights
| ğŸ’—ğŸ’—ğŸ’— | What makes Slow Fast Sampling special? |
|----|-------------------------|
| **Three Golden Principles** ğŸ‘‘ |***Certainty*, *Convergence*, *Positional*** guide exactly *when* and *where* to decode. |
| **Two-Stage Dance** ğŸ¢â†’âš¡ | Cautious ***Slow*** phase finds a stable span, then the ***Fast*** phase parallel-decodes it in one swoop. |
| **Plug-and-Play** ğŸ”Œ | Drop-in sampler for any *masked-diffusion LLM*: LLaDA-8B, Dream-7B. |
| **Crazy Speed-ups** âš¡ | **15.6 Ã—** faster than vanilla diffusion; **34.2 Ã—** with [`dLLM-Cache`](https://github.com/maomaocun/dLLM-Cache) â€”with *minimal* accuracy loss.  |
| **Outruns ARMs** ğŸƒ | Beats LLaMA-3 8B in throughput while matching accuracy (Table 4, p. 9). |

---

## ğŸš€ Pipeline at a Glance
<div align="center">

![SFS-overview](assets/method.png)  
**Fig. 2 â€“ Overview of the Slow Fast Sampling Pipeline: From Exploratory to Accelerated Decoding.**

</div>

---

## ğŸ› ï¸ Installation
```bash
# 1. Clone
git clone https://github.com/LiangrunFlora/Slow-Fast-Sampling.git
cd slow-fast-sampling

# 2. Env (Python â‰¥ 3.10) & Deps
bash install.sh         
```
## ğŸ“˜ Quick Start
```bash
# GSM8K with LLaDA-8B
bash scripts/run_llada_gsm8k_base.sh

# GPQA with LLaDA-8B
bash scripts/run_llada_gpqa_base.sh

# BBH with Dream-7B
bash scripts/run_dream_bbh_base.sh
```

## ğŸ“® Contact
Created and maintained by Qingyan Wei (liangrun@csu.edu.cn).
Feel free to open an issue or drop me an emailâ€”PRs are welcome!

## ğŸ‰ Acknowledgements
This project stands on the shoulders of [LLaDA](https://github.com/ML-GSAI/LLaDA), [Dream](https://github.com/HKUNLP/Dream), [dLLM-Cache](https://github.com/maomaocun/dLLM-Cache) and the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness).
Huge thanks to these amazing communities for paving the way.

## ğŸ“Œ Citation

If you find this work useful, please cite our paper:

```bibtex
@article{wei2025accelerating,
  title={Accelerating Diffusion Large Language Models with SlowFast: The Three Golden Principles},
  author={Wei, Qingyan and Zhang, Yaojie and Liu, Zhiyuan and Liu, Dongrui and Zhang, Linfeng},
  journal={arXiv preprint arXiv:2506.10848},
  year={2025}
}
```